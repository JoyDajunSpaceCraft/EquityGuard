{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c45d8e2-8cc4-4da5-bf32-d11a7cf9ffef",
   "metadata": {},
   "source": [
    "## Trec dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96172551-db61-4e04-939e-b65629a12cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_trec_2021 = \"../trec_2021/retrieved_trials.json\"\n",
    "retrieval_trec_2022 = \"../trec_2022/retrieved_trials.json\"\n",
    "retrieval_sigir = \"../sigir/retrieved_trials.json\"\n",
    "import json\n",
    "with open(retrieval_trec_2021, \"r\") as f1,open(retrieval_trec_2022, \"r\") as f2,open(retrieval_sigir, \"r\") as f3:\n",
    "    trec_2021_query2note = json.load(f1)\n",
    "    trec_2022_query2note = json.load(f2)\n",
    "    sigir_query2note = json.load(f3)\n",
    "\n",
    "# sigir_query2note\n",
    "# [{'patient_id': 'sigir-20141',\n",
    "#   'patient': 'A 58-year-old African-American woman presents to the ER with episodic pressing/burning anterior chest pain that began two days earlier for the first time in her life. The pain started while she was walking, radiates to the back, and is accompanied by nausea, diaphoresis and mild dyspnea, but is not increased on inspiration. The latest episode of pain ended half an hour prior to her arrival. She is known to have hypertension and obesity. She denies smoking, diabetes, hypercholesterolemia, or a family history of heart disease. She currently takes no medications. Physical examination is normal. The EKG shows nonspecific changes.',\n",
    "#   '0': [{'brief_title': 'Usefulness of Chest Wall Tenderness as Bedside Test to Exclude Acute Coronary Syndrome in Different Demographic Groups',\n",
    "#     'phase': '',\n",
    "#     'drugs': \"['Clinical examination: chest wall tenderness']\",\n",
    "#     'drugs_list': ['Clinical examination: chest wall tenderness'],\n",
    "#     'diseases': \"['Chest Pain']\",\n",
    "#     'diseases_list': ['Chest Pain'],\n",
    "#     'enrollment': '110.0',\n",
    "#     'inclusion_criteria': 'inclusion criteria: All patients over the age of 18 years presenting with the leading symptom of first time or recurrent acute chest pain in the emergency room of the Department of Internal Medicine, University Hospital of Zurich. \\n\\n ',\n",
    "#     'exclusion_criteria': ': \\n\\n Missing informed consent. \\n\\n Cardiopulmonary unstable patients. \\n\\n No self reported chest pain. \\n\\n Recent thoracic surgery within1 year, inflammatory joint disease, fibromyalgia, cardiogenic shock.',\n",
    "#     'brief_summary': 'To determine the significance of a simple bedside clinical test (chest wall tenderness) to exclude myocardial ischemia in different demographic groups.',\n",
    "#     'NCTID': 'NCT01724996'},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cbb2ab4-d6ad-4da8-823d-612c7ed32f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(sigir_query2note) # 58\n",
    "# len(trec_2021_query2note) #75\n",
    "# len(trec_2022_query2note) # 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c949d-bbb5-42b5-8cf5-0e96ddc3ed2f",
   "metadata": {},
   "source": [
    "## Detected the sensitive information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba888525-54a7-4519-beb4-c0fcf94651c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /jet/home/yji3/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /jet/home/yji3/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /jet/home/yji3/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import os \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import re\n",
    "# Only download once\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "sensitive_words = \"../sensitive_list\"\n",
    "files = os.listdir(sensitive_words)\n",
    "sensitive_list = []\n",
    "for file in files:\n",
    "    path = os.path.join(sensitive_words, file)\n",
    "    with open(path, \"r\") as f:\n",
    "        for i in f.readlines():\n",
    "            sensitive_list.append(i.split(\"\\n\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0460746e-e0ac-4f2d-b3c1-2cc09fbfc29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "sensitive_categories = {\n",
    "    'gender': {\n",
    "        \"he\": \"she\",\n",
    "        \"she\": \"he\",\n",
    "        \"his\": \"her\",\n",
    "        \"her\": \"his\",\n",
    "        \"him\": \"her\",\n",
    "        \"man\": \"woman\",\n",
    "        \"woman\": \"man\",\n",
    "        \"boy\": \"girl\",\n",
    "        \"girl\": \"boy\",\n",
    "        \"male\": \"female\",\n",
    "        \"female\": \"male\",\n",
    "        \"husband\": \"wife\",\n",
    "        \"wife\": \"husband\",\n",
    "    },\n",
    "    'ethnicity': {\n",
    "        \"african-american\": \"white\",\n",
    "        \"asian\": \"white\",\n",
    "        \"hispanic\": \"white\",\n",
    "        \"latino\": \"white\",\n",
    "        \"latina\": \"white\",\n",
    "        \"black\": \"white\",\n",
    "        # \"white\": \"black\",\n",
    "        \"caucasian\": \"black\",\n",
    "        \"native american\": \"white\",\n",
    "        \"indian\": \"white\",\n",
    "        \"middle eastern\": \"white\",\n",
    "        \"pacific islander\": \"white\",\n",
    "        \"multiracial\": \"white\",\n",
    "        \"biracial\": \"white\",\n",
    "        \"jewish\": \"white\",\n",
    "        \"arab\": \"white\",\n",
    "        \"african\": \"white\",\n",
    "        \"european\": \"black\",\n",
    "        \"chinese\": \"white\",\n",
    "        \"japanese\": \"white\",\n",
    "        \"korean\": \"white\",\n",
    "        \"filipino\": \"white\",\n",
    "        \"vietnamese\": \"white\",\n",
    "        \"thai\": \"white\",\n",
    "        \"cambodian\": \"white\",\n",
    "        \"lao\": \"white\",\n",
    "        \"hmong\": \"white\",\n",
    "        \"french\": \"white\",\n",
    "        \"german\": \"white\",\n",
    "        \"italian\": \"white\",\n",
    "        \"irish\": \"white\",\n",
    "        \"russian\": \"white\",\n",
    "        \"polish\": \"white\",\n",
    "        \"greek\": \"white\",\n",
    "        \"portuguese\": \"white\",\n",
    "        \"spanish\": \"white\",\n",
    "        \"swedish\": \"white\",\n",
    "        \"norwegian\": \"white\",\n",
    "        \"dutch\": \"white\",\n",
    "        \"belgian\": \"white\",\n",
    "        \"swiss\": \"white\",\n",
    "        \"austrian\": \"white\",\n",
    "        \"danish\": \"white\",\n",
    "        \"finnish\": \"white\",\n",
    "        \"hungarian\": \"white\",\n",
    "        \"romanian\": \"white\",\n",
    "        \"bulgarian\": \"white\",\n",
    "        \"yugoslav\": \"white\",\n",
    "        \"slovak\": \"white\",\n",
    "        \"czech\": \"white\",\n",
    "        \"ukrainian\": \"white\",\n",
    "        \"latvian\": \"white\",\n",
    "        \"estonian\": \"white\",\n",
    "        \"lithuanian\": \"white\",\n",
    "        \"armenian\": \"white\",\n",
    "        \"georgian\": \"white\",\n",
    "        \"mongolian\": \"white\",\n",
    "        \"turkish\": \"white\",\n",
    "        \"persian\": \"white\",\n",
    "        \"iraqi\": \"white\",\n",
    "        \"syrian\": \"white\",\n",
    "        \"lebanese\": \"white\",\n",
    "        \"egyptian\": \"white\",\n",
    "        \"moroccan\": \"white\",\n",
    "        \"algerian\": \"white\",\n",
    "        \"tunisian\": \"white\",\n",
    "        \"libyan\": \"white\",\n",
    "        \"ethiopian\": \"white\",\n",
    "        \"somali\": \"white\",\n",
    "        \"sudanese\": \"white\",\n",
    "        \"kenyan\": \"white\",\n",
    "        \"tanzanian\": \"white\",\n",
    "        \"ugandan\": \"white\",\n",
    "        \"rwandan\": \"white\",\n",
    "        \"burundian\": \"white\",\n",
    "        \"zambian\": \"white\",\n",
    "        \"zimbabwean\": \"white\",\n",
    "        \"namibian\": \"white\",\n",
    "        \"botswanan\": \"white\",\n",
    "        \"swazi\": \"white\",\n",
    "        \"lesotho\": \"white\",\n",
    "        \"mozambican\": \"white\",\n",
    "        \"madagascan\": \"white\",\n",
    "        \"mauritian\": \"white\",\n",
    "        \"seychellois\": \"white\",\n",
    "        \"australian\": \"white\",\n",
    "        \"new zealander\": \"white\",\n",
    "        \"papua new guinean\": \"white\",\n",
    "        \"fijian\": \"white\",\n",
    "        \"samoan\": \"white\",\n",
    "        \"tongan\": \"white\",\n",
    "        \"tuvaluan\": \"white\",\n",
    "        \"kiribati\": \"white\",\n",
    "        \"marshallese\": \"white\",\n",
    "        \"micronesian\": \"white\",\n",
    "        \"palauan\": \"white\",\n",
    "        \"nauruan\": \"white\",\n",
    "        \"vanuatuan\": \"white\",\n",
    "        \"solomon islander\": \"white\",\n",
    "        \"new caledonian\": \"white\",\n",
    "        \"french polynesian\": \"white\",\n",
    "        \"wallisian\": \"white\",\n",
    "        \"futunan\": \"white\",\n",
    "        \"american samoa\": \"white\",\n",
    "        \"guamanian\": \"white\",\n",
    "        \"northern mariana islander\": \"white\",\n",
    "        \"hokkaido\": \"white\",\n",
    "        \"okinawan\": \"white\",\n",
    "    \n",
    "        # \"african-american\": \"white\",\n",
    "        # \"asian\": \"white\",\n",
    "        # \"hispanic\": \"white\"\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def get_replacement(word, category):\n",
    "    if word.lower() in sensitive_categories[category]:\n",
    "        return {sensitive_categories[category][word.lower()]}\n",
    "    \n",
    "    # antonyms = set()\n",
    "    # for syn in wordnet.synsets(word):\n",
    "    #     for lemma in syn.lemmas():\n",
    "    #         if lemma.antonyms():\n",
    "    #             antonyms.add(lemma.antonyms()[0].name())\n",
    "    # return antonyms\n",
    "\n",
    "\n",
    "def replace_with_replacements(phrase, category):\n",
    "    words = phrase.split()\n",
    "    replaced_words = []\n",
    "    for word in words:\n",
    "        replacements = get_replacement(word, category)\n",
    "        if replacements:\n",
    "            replaced_words.append(list(replacements)[0])\n",
    "        else:\n",
    "            replaced_words.append(word)\n",
    "    return ' '.join(replaced_words)\n",
    "\n",
    "\n",
    "def replace_sensitive_phrases(text, sensitive_phrases, category):\n",
    "    for phrase in sorted(sensitive_phrases, key=len, reverse=True):\n",
    "        replacement_phrase = replace_with_replacements(phrase, category)\n",
    "        text = re.sub(r'\\b' + re.escape(phrase) + r'\\b', replacement_phrase, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "\n",
    "def classify_sensitive_phrases(sensitive_phrases):\n",
    "    classified_phrases = {'gender': [], 'ethnicity': []}\n",
    "    for phrase in sensitive_phrases:\n",
    "        tokens = phrase.split()\n",
    "        for token in tokens:\n",
    "            for category in sensitive_categories:\n",
    "                if token.lower() in sensitive_categories[category]:\n",
    "                    # print(\"token\", token)\n",
    "                    # print(\"phrase\", phrase)\n",
    "                    classified_phrases[category].append(token)\n",
    "                    break\n",
    "    return classified_phrases\n",
    "\n",
    "\n",
    "def find_sensitive_phrases(tokens, sensitive_words):\n",
    "    detected_sensitive_words = set()\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    lower_sensitive_words = set([word.lower() for word in sensitive_words])\n",
    "\n",
    "\n",
    "    max_window_size = 2\n",
    "    \n",
    "    for window_size in range(1, max_window_size + 1):\n",
    "        for i in range(token_count - window_size + 1):\n",
    "            window_tokens = tokens[i:i + window_size]\n",
    "            window_phrase = \" \".join(window_tokens).lower()\n",
    "            \n",
    "\n",
    "            if any(word in window_phrase.split() for word in lower_sensitive_words):\n",
    "                detected_sensitive_words.add(window_phrase)\n",
    "    \n",
    "    return list(detected_sensitive_words)\n",
    "\n",
    "def replace_all_sensitive_categories(patient_note, classified_phrases, categories_to_replace=None):\n",
    "    new_patient_note = copy.deepcopy(patient_note)\n",
    "    if categories_to_replace is None:\n",
    "        categories_to_replace = sensitive_categories.keys()\n",
    "    \n",
    "    for category in categories_to_replace:\n",
    "        new_patient_note = replace_sensitive_phrases(new_patient_note, classified_phrases[category], category)\n",
    "        # print(\"patient_note inside\", new_patient_note)\n",
    "    return new_patient_note\n",
    "\n",
    "\n",
    "res = {}\n",
    "for query2note in sigir_query2note:\n",
    "    patient_id = query2note[\"patient_id\"]\n",
    "    res[patient_id] = {}\n",
    "    \n",
    "    patient_note = query2note[\"patient\"]\n",
    "    res[patient_id][\"patient\"] = patient_note\n",
    "    clinical_trials = query2note[\"patient\"]\n",
    "    tokens = word_tokenize(patient_note)\n",
    "    sensitive = find_sensitive_phrases(tokens,sensitive_list) # ['she', 'her', 'in her life', '58-year-old african-american woman', 'to her arrival',...]\n",
    "    classified_phrases = classify_sensitive_phrases(sensitive)\n",
    "    categories_to_replace = ['gender', 'ethnicity']\n",
    "    \n",
    "    new_text = replace_all_sensitive_categories(patient_note, classified_phrases, categories_to_replace)\n",
    "    res[patient_id][\"gender\"] = replace_all_sensitive_categories(patient_note, classified_phrases, [\"gender\"]) if classified_phrases[\"gender\"]!=[] else None\n",
    "    res[patient_id][\"ethnicity\"] = replace_all_sensitive_categories(patient_note, classified_phrases, [\"ethnicity\"]) if classified_phrases[\"ethnicity\"]!=[] else None\n",
    "    res[patient_id][\"all\"] = replace_all_sensitive_categories(patient_note, classified_phrases,categories_to_replace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d997f98d-b3f5-4c1a-b537-b87f1b593a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"sigir_sensitive.json\", \"w\") as f:\n",
    "    json.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48721f20-f302-44c9-915c-fada32403fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for query2note in trec_2022_query2note:\n",
    "    patient_id = query2note[\"patient_id\"]\n",
    "    res[patient_id] = {}\n",
    "    \n",
    "    patient_note = query2note[\"patient\"]\n",
    "    res[patient_id][\"patient\"] = patient_note\n",
    "    clinical_trials = query2note[\"patient\"]\n",
    "    tokens = word_tokenize(patient_note)\n",
    "    sensitive = find_sensitive_phrases(tokens,sensitive_list) # ['she', 'her', 'in her life', '58-year-old african-american woman', 'to her arrival',...]\n",
    "    classified_phrases = classify_sensitive_phrases(sensitive)\n",
    "    categories_to_replace = ['gender', 'ethnicity']\n",
    "    \n",
    "    new_text = replace_all_sensitive_categories(patient_note, classified_phrases, categories_to_replace)\n",
    "    res[patient_id][\"gender\"] = replace_all_sensitive_categories(patient_note, classified_phrases, [\"gender\"]) if classified_phrases[\"gender\"]!=[] else None\n",
    "    res[patient_id][\"ethnicity\"] = replace_all_sensitive_categories(patient_note, classified_phrases, [\"ethnicity\"]) if classified_phrases[\"ethnicity\"]!=[] else None\n",
    "    res[patient_id][\"all\"] = replace_all_sensitive_categories(patient_note, classified_phrases,categories_to_replace)\n",
    "import json\n",
    "with open(\"trec_2022_sensitive.json\", \"w\") as f:\n",
    "    json.dump(res, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e16fac-0c37-458b-9f64-52f7d33ad05b",
   "metadata": {},
   "source": [
    "## Use TrialGPT to verify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c7ec59-1756-42aa-8a0e-129c1efbe762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with the already defined three already defined dataset\n",
    "# Add openai key\n",
    "with open(\"OpenaiKey.txt\", \"r\")as f:\n",
    "    item = f.read()\n",
    "item = item.strip()\n",
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = item\n",
    "client = OpenAI()\n",
    "# response = client.chat.completions.create(\n",
    "#   model=\"gpt-4\",\n",
    "#   messages=[\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "#   ]\n",
    "# )\n",
    "# response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727c127a-6872-41aa-b43b-795b0a93a2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43f3ee-03cc-4c74-b35f-a4cf25cb0a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TrialGPT import trialgpt_matching\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import sys\n",
    "corpuses = [\"sigir\", \"trec_2021\", \"trec_2022\"]\n",
    "model = \"gpt-4\"\n",
    "def process_patient(patient):\n",
    "    sents = sent_tokenize(patient)\n",
    "    sents.append(\"The patient will provide informed consent, and will comply with the trial protocol without any practical issues.\")\n",
    "    sents = [f\"{idx}. {sent}\" for idx, sent in enumerate(sents)]\n",
    "    patient = \"\\n\".join(sents)\n",
    "    return patient\n",
    "for corpus in corpuses:\n",
    "    original_dataset = json.load(open(f\"../{corpus}/retrieved_trials.json\"))\n",
    "    \n",
    "    new_patient_note = json.load(open(f\"{corpus}_sensitive.json\"))\n",
    "\n",
    "    output_path = f\"fairness_results/matching_results_{corpus}_{model}.json\" \n",
    "\n",
    "    # Dict{Str(patient_id): Dict{Str(label): Dict{Str(trial_id): Str(output)}}}\n",
    "    if os.path.exists(output_path):\n",
    "        output = json.load(open(output_path))\n",
    "    else:\n",
    "        output = {}\n",
    "         \n",
    "        \n",
    "\n",
    "    for instance in original_dataset[:20]:  # here I limit every label only using 20\n",
    "        # Dict{'patient': Str(patient), '0': Str(NCTID), ...}\n",
    "        patient_id = instance[\"patient_id\"]\n",
    "        patient = instance[\"patient\"]\n",
    "        sensitive = new_patient_note[patient_id]\n",
    "        cat_sensitive = {}\n",
    "        for k, v in sensitive.items():\n",
    "            if v!=None and k!=\"patient\":\n",
    "                cat_sensitive[k] = process_patient(v)\n",
    "        # same process \n",
    "        original_patient = process_patient(patient)\n",
    "        \n",
    "        \n",
    "\n",
    "        # initialize the patient id in the output \n",
    "        if patient_id not in output:\n",
    "            output[patient_id] = {}\n",
    "            for sensitive_type, _ in cat_sensitive.items():\n",
    "                output[patient_id][sensitive_type] = {\"0\": {}, \"1\": {}, \"2\": {}}\n",
    "        \n",
    "        \n",
    "        # only use the sensitive changed as the input patient note\n",
    "        for sensitive_type, sensitive_note in cat_sensitive.items():\n",
    "            for label in [\"2\", \"1\", \"0\"]:\n",
    "                if label not in instance: continue\n",
    "                \n",
    "                for trial in instance[label][:10]:  # here I limit every label only using 30\n",
    "                    trial_id = trial[\"NCTID\"]\n",
    "                    \n",
    "                    # already calculated and cached\n",
    "                    if trial_id in output[patient_id][sensitive_type][label]:\n",
    "                        continue\n",
    "\n",
    "                    # in case anything goes wrong (e.g., API calling errors)\n",
    "                    try:\n",
    "                        results = trialgpt_matching(trial, sensitive_note, model, client)\n",
    "                        # results = \"sdsa\"\n",
    "                        output[patient_id][sensitive_type][label][trial_id] = results\n",
    "                        # output[patient_id][]\n",
    "                        with open(output_path, \"w\") as f:\n",
    "                            json.dump(output, f, indent=4)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
