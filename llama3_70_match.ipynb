{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624fe0f2-0b91-4ffc-8d72-97bb4aa565e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"/ocean/projects/med230010p/yji3/llama3_70B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"/ocean/projects/med230010p/yji3/llama3_70B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552ef0f6-485b-4126-9230-ca2a9dbad807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformers\n",
      "Version: 4.43.0\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /jet/home/yji3/.local/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: llama-recipes, optimum, peft, pyserini, sentence-transformers, transformer-lens\n"
     ]
    }
   ],
   "source": [
    "!pip show transformers\n",
    "!pip install transformers==4.43.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a26553-60a3-4307-969f-453adebc2f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 30/30 [20:32<00:00, 41.07s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\n",
    "\n",
    "model_id=\"/ocean/projects/med230010p/yji3/llama3_70B\"\n",
    "\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a804da1-fc3e-4c0f-8cb0-f249a729704b",
   "metadata": {},
   "source": [
    "# LLAMA 70 B for matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e97997f7-4361-4d5f-9190-b9beed7ffb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95176785-92d1-4613-915f-7b4b041d7a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\n",
    "\n",
    "model_id=\"/ocean/projects/med230010p/yji3/llama3_70B\"\n",
    "\n",
    "def parse_criteria(criteria):\n",
    "    output = \"\"\n",
    "    criteria = criteria.split(\"\\n\\n\")\n",
    "    \n",
    "    idx = 0\n",
    "    for criterion in criteria:\n",
    "        criterion = criterion.strip()\n",
    "\n",
    "        if \"inclusion criteria\" in criterion.lower() or \"exclusion criteria\" in criterion.lower():\n",
    "            continue\n",
    "\n",
    "        if len(criterion) < 5:\n",
    "            continue\n",
    "    \n",
    "        output += f\"{idx}. {criterion}\\n\" \n",
    "        idx += 1\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def print_trial(\n",
    "    trial_info: dict,\n",
    "    inc_exc: str,\n",
    ") -> str:\n",
    "    \"\"\"Given a dict of trial information, returns a string of trial.\"\"\"\n",
    "    \n",
    "    trial = f\"Title: {trial_info['brief_title']}\\n\"\n",
    "    trial += f\"Target diseases: {', '.join(trial_info['diseases_list'])}\\n\"\n",
    "    trial += f\"Interventions: {', '.join(trial_info['drugs_list'])}\\n\"\n",
    "    trial += f\"Summary: {trial_info['brief_summary']}\\n\"\n",
    "    \n",
    "    if inc_exc == \"inclusion\":\n",
    "        trial += \"Inclusion criteria:\\n %s\\n\" % parse_criteria(trial_info['inclusion_criteria'])\n",
    "    elif inc_exc == \"exclusion\":\n",
    "        trial += \"Exclusion criteria:\\n %s\\n\" % parse_criteria(trial_info['exclusion_criteria']) \n",
    "\n",
    "    return trial\n",
    "\n",
    "\n",
    "def get_matching_prompt(\n",
    "    trial_info: dict,\n",
    "    inc_exc: str,\n",
    "    patient: str,\n",
    ") -> str:\n",
    "    \"\"\"Output the prompt.\"\"\"\n",
    "    prompt = f\"You are a helpful assistant for clinical trial recruitment. Your task is to compare a given patient note and the {inc_exc} criteria of a clinical trial to determine the patient's eligibility at the criterion level.\\n\"\n",
    "\n",
    "    if inc_exc == \"inclusion\":\n",
    "        prompt += \"The factors that allow someone to participate in a clinical study are called inclusion criteria. They are based on characteristics such as age, gender, the type and stage of a disease, previous treatment history, and other medical conditions.\\n\"\n",
    "    \n",
    "    elif inc_exc == \"exclusion\":\n",
    "        prompt += \"The factors that disqualify someone from participating are called exclusion criteria. They are based on characteristics such as age, gender, the type and stage of a disease, previous treatment history, and other medical conditions.\\n\"\n",
    "\n",
    "    prompt += f\"You should check the {inc_exc} criteria one-by-one, and output the following three elements for each criterion:\\n\"\n",
    "    prompt += f\"\\tElement 1. For each {inc_exc} criterion, briefly generate your reasoning process: First, judge whether the criterion is not applicable (not very common), where the patient does not meet the premise of the criterion. Then, check if the patient note contains direct evidence. If so, judge whether the patient meets or does not meet the criterion. If there is no direct evidence, try to infer from existing evidence, and answer one question: If the criterion is true, is it possible that a good patient note will miss such information? If impossible, then you can assume that the criterion is not true. Otherwise, there is not enough information.\\n\"\n",
    "    prompt += f\"\\tElement 2. If there is relevant information, you must generate a list of relevant sentence IDs in the patient note. If there is no relevant information, you must annotate an empty list.\\n\" \n",
    "    prompt += f\"\\tElement 3. Classify the patient eligibility for this specific {inc_exc} criterion: \"\n",
    "    \n",
    "    if inc_exc == \"inclusion\":\n",
    "        prompt += 'the label must be chosen from {\"not applicable\", \"not enough information\", \"included\", \"not included\"}. \"not applicable\" should only be used for criteria that are not applicable to the patient. \"not enough information\" should be used where the patient note does not contain sufficient information for making the classification. Try to use as less \"not enough information\" as possible because if the note does not mention a medically important fact, you can assume that the fact is not true for the patient. \"included\" denotes that the patient meets the inclusion criterion, while \"not included\" means the reverse.\\n'\n",
    "    elif inc_exc == \"exclusion\":\n",
    "        prompt += 'the label must be chosen from {\"not applicable\", \"not enough information\", \"excluded\", \"not excluded\"}. \"not applicable\" should only be used for criteria that are not applicable to the patient. \"not enough information\" should be used where the patient note does not contain sufficient information for making the classification. Try to use as less \"not enough information\" as possible because if the note does not mention a medically important fact, you can assume that the fact is not true for the patient. \"excluded\" denotes that the patient meets the exclusion criterion and should be excluded in the trial, while \"not excluded\" means the reverse.\\n'\n",
    "    \n",
    "    prompt += \"You should output only a JSON dict exactly formatted as: dict{str(criterion_number): list[str(element_1_brief_reasoning), list[int(element_2_sentence_id)], str(element_3_eligibility_label)]}.\"\n",
    "    \n",
    "    user_prompt = f\"Here is the patient note, each sentence is led by a sentence_id:\\n{patient}\\n\\n\" \n",
    "    user_prompt += f\"Here is the clinical trial:\\n{print_trial(trial_info, inc_exc)}\\n\\n\"\n",
    "    user_prompt += f\"Plain JSON output:\"\n",
    "\n",
    "    return prompt, user_prompt\n",
    "\n",
    "\n",
    "def trialgpt_matching(trial: dict, patient: str):\n",
    "    results = {}\n",
    "    # hard code model\n",
    "    # model = LlamaForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', torch_dtype=torch.float16)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # doing inclusions and exclusions in separate prompts\n",
    "    for inc_exc in [\"inclusion\", \"exclusion\"]:\n",
    "        system_prompt, user_prompt = get_matching_prompt(trial, inc_exc, patient)\n",
    "        eval_prompt = f\"\"\"\n",
    "           content: {system_prompt},\n",
    "           content\": {user_prompt},\n",
    "        \"\"\"\n",
    "        model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            res = tokenizer.decode(model.generate(**model_input, max_new_tokens=200,pad_token_id=tokenizer.eos_token_id)[0], skip_special_tokens=True)\n",
    "            message = res[len(eval_prompt):]\n",
    "\n",
    "        try:\n",
    "            results[inc_exc] = json.loads(message)\n",
    "        except:\n",
    "            results[inc_exc] = message\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28216a-9f3b-4eab-bad9-4fa9286780e3",
   "metadata": {},
   "source": [
    "# inference matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fc586a-73b2-422c-a79d-ae0ac40c98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from trialgpt import trialgpt_matching\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import sys\n",
    "# corpuses = [\"sigir\", \"trec_2021\", \"trec_2022\"]\n",
    "corpuses = [\"trec_2022\"]\n",
    "\n",
    "\n",
    "def process_patient(patient):\n",
    "    sents = sent_tokenize(patient)\n",
    "    sents.append(\"The patient will provide informed consent, and will comply with the trial protocol without any practical issues.\")\n",
    "    sents = [f\"{idx}. {sent}\" for idx, sent in enumerate(sents)]\n",
    "    patient = \"\\n\".join(sents)\n",
    "    return patient\n",
    "for corpus in corpuses:\n",
    "    original_dataset = json.load(open(f\"{corpus}/retrieved_trials.json\"))\n",
    "    model_name = \"llama\"\n",
    "    # gender_race_patient_note = json.load(open(f\"sensitive_changed_patient_note/{corpus}_sensitive.json\"))\n",
    "    social_patient_note = json.load(open(f\"sensitive_changed_patient_note/{corpus}_all_sensitive.json\"))\n",
    "    output_path = f\"fairness_results/matching_results_{corpus}_{model_name}.json\" \n",
    "\n",
    "    # Dict{Str(patient_id): Dict{Str(label): Dict{Str(trial_id): Str(output)}}}\n",
    "    if os.path.exists(output_path):\n",
    "        output = json.load(open(output_path))\n",
    "    else:\n",
    "        output = {}   \n",
    "    for instance in original_dataset: \n",
    "        # Dict{'patient': Str(patient), '0': Str(NCTID), ...}\n",
    "        patient_id = instance[\"patient_id\"]\n",
    "        patient = instance[\"patient\"]\n",
    "        sensitive = social_patient_note[patient_id]\n",
    "        cat_sensitive = {}\n",
    "        for k, v in sensitive.items():\n",
    "            # if v!=None and k!=\"patient\":\n",
    "            if v!=None:\n",
    "                cat_sensitive[k] = process_patient(v)\n",
    "        # same process \n",
    "        original_patient = process_patient(patient)\n",
    "        \n",
    "        # initialize the patient id in the output \n",
    "        if patient_id not in output:\n",
    "            output[patient_id] = {}\n",
    "            for sensitive_type, _ in cat_sensitive.items(): # this time I also added the normal \n",
    "                output[patient_id][sensitive_type] = {\"0\": {}, \"1\": {}, \"2\": {}}\n",
    "        \n",
    "        \n",
    "        # only use the sensitive changed as the input patient note\n",
    "        for sensitive_type, sensitive_note in cat_sensitive.items():\n",
    "            for label in [\"2\", \"1\", \"0\"]:\n",
    "                if label not in instance: continue\n",
    "                \n",
    "                for trial in instance[label]:  # here I limit every label only using 30\n",
    "                    trial_id = trial[\"NCTID\"]\n",
    "                    \n",
    "                    # already calculated and cached\n",
    "                    if trial_id in output[patient_id][sensitive_type][label]:\n",
    "                        continue\n",
    "\n",
    "                    # in case anything goes wrong (e.g., API calling errors)\n",
    "                    # try:\n",
    "                    results = trialgpt_matching(trial, sensitive_note)\n",
    "                    # results = \"sdsa\"\n",
    "                    output[patient_id][sensitive_type][label][trial_id] = results\n",
    "                    # output[patient_id][]\n",
    "                    with open(output_path, \"w\") as f:\n",
    "                        json.dump(output, f, indent=4)\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afb85df-4ca9-40aa-9257-f045cae18be6",
   "metadata": {},
   "source": [
    "## Generate ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f3b04-7539-4bf2-af49-ce54cd526533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c1f2f-f5ff-401d-91da-294bae1946f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NGC PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
